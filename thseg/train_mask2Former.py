# -*- coding: utf-8 -*-
"""segformer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_t3KvF3qg4IJfEhTuftFI1GSlscapNgf
"""


from torch.utils.data import Dataset, DataLoader
import torch
from torch import nn
from sklearn.metrics import accuracy_score
from tqdm import tqdm
import os, sys
from PIL import Image
from transformers import PretrainedConfig, Mask2FormerImageProcessor, AutoImageProcessor, Mask2FormerForUniversalSegmentation, MaskFormerForInstanceSegmentation, Mask2FormerConfig, Mask2FormerModel
import pandas as pd
import cv2
import numpy as np
import albumentations as aug
import pdb
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
from tools.metrics import ConfusionMatrix, IntersectionOverUnion, OverallAccuracy, F1Score, get_acc_v2
from tools.parse_config_yaml import parse_yaml
import tools.transform as tr
import torchvision.transforms as standard_transforms
from timm.utils import AverageMeter




def get_input_grad(model, samples):
    outputs = model(samples).logits
    out_size = outputs.size()
    central_point = torch.nn.functional.relu(outputs[:, :, out_size[2] // 2, out_size[3] // 2]).sum()
    grad = torch.autograd.grad(central_point, samples)
    grad = grad[0]
    grad = torch.nn.functional.relu(grad)
    aggregated = grad.sum((0, 1))
    grad_map = aggregated.cpu().numpy()

    return grad_map

    
def visualize_attention(attentions):
    num_heads = attentions.shape[0]
    num_tokens = attentions.shape[1]


    #reshape(batch_size, -1, height, width)
    
    attentions = attentions.view(num_heads, num_tokens, num_tokens)

    # Normalize the attention weights across the sequence
    attentions = torch.softmax(attentions, dim=0)
    #pdb.set_trace()
    #attentions = attentions.reshape(num_heads, 512, 512)

    # Create subplots for each head
    fig, axs = plt.subplots(num_heads, 1, figsize=(8, 4*num_heads))
    if num_heads == 1:
        axs = [axs]  # Handle the case of a single head

    # Plot the attention maps
    for head in range(num_heads):
        ax = axs[head]
        ax.imshow(attentions[head].detach().cpu().numpy(), cmap='inferno', interpolation='nearest')

        # Customize the plot
        #ax.set_xticks(range(num_tokens))
        #ax.set_yticks(range(num_tokens))
        #ax.set_xticklabels(range(1, num_tokens+1))
        #ax.set_yticklabels(range(1, num_tokens+1))
        ax.set_xlabel('Input Tokens')
        ax.set_ylabel('Output Tokens')
        ax.set_title(f'Head {head+1} Attention')

    # Adjust spacing between subplots
    plt.tight_layout()

    # Show the plot
    plt.show()

class EarlyStopper:
    def __init__(self, patience=1, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.min_validation_iou = -1000

    def early_stop(self, validation_iou):
        if validation_iou > self.min_validation_iou:
            self.min_validation_iou = validation_iou
            self.counter = 0
        elif validation_iou < (self.min_validation_iou + self.min_delta):
            self.counter += 1
            if self.counter >= self.patience:
                return True
        return False

class ImageSegmentationDataset(Dataset):
    """Image segmentation dataset."""

    def __init__(self, root_dir, transforms=None, train=True, inference= False, g_vars =None):
        """
        Args:
            root_dir (string): Root directory of the dataset containing the images + annotations.
            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.
            train (bool): Whether to load "training" or "validation" images + annotations.
        """
        self.root_dir = root_dir
        self.train = train
        self.transforms = transforms

        if not inference:
            sub_path = "train" if self.train else "val"
        else:
            sub_path = "test"

        self.img_dir = os.path.join(self.root_dir, sub_path, "images")
        self.ann_dir = os.path.join(self.root_dir, sub_path, "labels") #"occ_masks") #"labels") #")#"occ_masks")
        
        # read images
        image_file_names = []
        for root, dirs, files in os.walk(self.img_dir):
            image_file_names.extend(files)
        self.images = sorted(image_file_names)
        
        # read annotations
        annotation_file_names = []
        for root, dirs, files in os.walk(self.ann_dir):
            annotation_file_names.extend(files)
        self.annotations = sorted(annotation_file_names)

        if g_vars is not None:
            g_vars.set_var(self.annotations)

        assert len(self.images) == len(self.annotations), "There must be as many images as there are segmentation maps"

    def __len__(self):
        return len(self.images)
    
    def get_annotations(self):
        return self.annotations

    def __getitem__(self, idx):
        
        image = cv2.imread(os.path.join(self.img_dir, self.images[idx]))
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        segmentation_map = cv2.imread(os.path.join(self.ann_dir, self.annotations[idx]))
        segmentation_map = cv2.cvtColor(segmentation_map, cv2.COLOR_BGR2GRAY)
        segmentation_map = np.where(segmentation_map>0, 1, 2) #1-window 2-wall
        #segmentation_map = segmentation_map[np.where((segmentation_map == [0, 0, 0]).all(axis=2))] = [255, 0, 0]

        sample = {'image': image, 'mask': segmentation_map}

        if self.transforms is not None:
            augmented = self.transforms(image=image, mask=segmentation_map) #Originally
            tr_image, tr_segmentation_map = augmented['image'], augmented['mask']
        else:
            tr_image= image
            tr_segmentation_map =segmentation_map

        
        # convert to C, H, W
        tr_image = image.transpose(2,0,1)
        
        return tr_image, tr_segmentation_map, image, segmentation_map


class global_var():
    def __init__(self):
        self.var = []

    def set_var(self, var):
        self.var = var

    def get_var(self):
        return self.var



def count_parameters(model):
            return sum(p.numel() for p in model.parameters() if p.requires_grad)

pre_model = "facebook/mask2former-swin-small-cityscapes-semantic" #"facebook/mask2former-swin-small-coco-semantic"
preprocessor = Mask2FormerImageProcessor(ignore_index=0, reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False) #
#preprocessor = AutoImageProcessor.from_pretrained(pre_model)

def collate_fn(batch):
    # Create a preprocessor
   

    inputs = list(zip(*batch))
    images = inputs[0]
    segmentation_maps = inputs[1]
    
    # this function pads the inputs to the same size,
    # and creates a pixel mask
    # actually padding isn't required here since we are cropping
    
    batch = preprocessor(
        images,
        segmentation_maps=segmentation_maps,
        return_tensors="pt",
    )
    
    batch["original_images"] = inputs[2]
    batch["original_segmentation_maps"] = inputs[3]
    
    return batch
    

def train():
    
    



    start_epoch = 1

    #root_dir = '/home/cero_ma/MCV/window_benchmarks/originals/resized/cmp-occluded60/'
    root_dir = "/home/cero_ma/MCV/window_benchmarks/originals/resized/ecp-ref-occ60/"

    train_dataset = ImageSegmentationDataset(root_dir=root_dir, transforms=transform)
    valid_dataset = ImageSegmentationDataset(root_dir=root_dir, transforms=transform, train=False)

    print("Number of training examples:", len(train_dataset))
    print("Number of validation examples:", len(valid_dataset))


    train_dataloader = DataLoader(train_dataset, batch_size=param_dict['batch_size'], shuffle=True, collate_fn=collate_fn)
    valid_dataloader = DataLoader(valid_dataset, batch_size=param_dict['batch_size'], collate_fn=collate_fn)

    #visible windows torch.tensor([0.60757092, 2.8240482])
    #occlusions [0.57487292, 3.83899089]
    criterion  = nn.CrossEntropyLoss(weight=torch.tensor([0.60757092, 2.8240482])).cuda()
    


    batch = next(iter(train_dataloader))
    
    for k,v in batch.items():
        if isinstance(v, torch.Tensor):
            print(k,v.shape)
        else:
            print(k,v[0].shape)
    



    #classes = pd.read_csv('/home/cero_ma/MCV/window_benchmarks/originals/resized/ecp-ref-occ60/train/occ-clases.csv')['name']
    #id2label = classes.to_dict()
    #label2id = {v: k for k, v in id2label.items()}

    
    #id2label = {1: "occlusion"}
    #label2id = {"occlusion": 1 }

    #model = SegformerForSemanticSegmentation.from_pretrained(pretrained_model_name_or_path = pre_model, ignore_mismatched_sizes=True, config = configuration_b2)
    
    model = Mask2FormerForUniversalSegmentation.from_pretrained(pretrained_model_name_or_path = pre_model, 
                                                          ignore_mismatched_sizes=True,
                                                          config = configuration)  # 
    print(model.config)
    
    if len(gpu_list) > 1:
        print('gpu>1')  
        model = torch.nn.DataParallel(model, device_ids=gpu_list)

    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)#, betas=(0.9, 0.999), weight_decay=0.01)
    #lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.8)#50 0.8) 20 0.1

    
    model.cuda()
    
    
    print("Model Initialized!")
    best_val_acc = 0.0
    running_loss = 0.0
    num_samples = 0
    val_running_loss = 0.0
    val_num_samples = 0
    print('Amount of Parameters: ', count_parameters(model))
    with open(os.path.join(param_dict['save_dir_model'], 'log.txt'), 'w') as ff:
        for epoch in range(start_epoch, param_dict['epoches']+1):  # loop over the dataset multiple times
            print("Epoch:", epoch)
            pbar = tqdm(train_dataloader)
            accuracies = []
            losses = []
            val_accuracies = []
            val_losses = []
            model.train()
            for idx, batch in enumerate(pbar):

                pdb.set_trace()
                
                # get the inputs;
                pixel_values = batch["pixel_values"].cuda()
                #labels = batch["labels"].cuda()
                
                mask_labels=[labels.cuda() for labels in batch["mask_labels"]]
                class_labels=[labels.cuda() for labels in batch["class_labels"]]

                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                #visual_mask = (batch["mask_labels"][0][0].bool().numpy() * 255).astype(np.uint8) # (1,512,512) for each image, there are N mask labels (N:classes)
                #cv2.imshow("f", visual_mask)

                #pixel_mask = (batch["pixel_mask"][0].bool().numpy() * 255).astype(np.uint8) #Original GT?
                #cv2.imshow("d", pixel_mask)
                #cv2.waitKey(0)
                #cv2.destroyAllWindows()

                
                
                
                
                outputs = model(pixel_values=pixel_values.float(), mask_labels=mask_labels, class_labels = class_labels)
                
                
                # Backward propagation
                loss = outputs.loss
                loss.backward()

                batch_size = batch["pixel_values"].size(0)
                running_loss += loss.item()
                num_samples += batch_size

                #loss torch.Size([1])
                #class_queries_logits torch.Size([3, 100, 2])
                #masks_queries_logits torch.Size([3, 100, 128, 128])

                #auxiliary_logits=None
                
                #encoder_last_hidden_state torch.Size([3, 768, 16, 16])
                #pixel_decoder_last_hidden_state torch.Size([3, 256, 128, 128])
                #transformer_decoder_last_hidden_state torch.Size([3, 100, 256])
                
                #encoder_hidden_states
                #pixel_decoder_hidden_states
                #transformer_decoder_hidden_states
                #attentions

                
                
                #visualize_attention(outputs.attentions[3][0])

                # evaluate
                original_images = batch["original_images"]
                
                target_sizes = [(image.shape[0], image.shape[1]) for image in original_images] 

                             
                predicted_segmentation_maps = preprocessor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes) #all predictions maps are 0
                predicted = torch.stack(predicted_segmentation_maps, dim=0)

                # get ground truth segmentation maps
                
                ground_truth_segmentation_maps = batch["original_segmentation_maps"] #list
                labels = np.asarray(ground_truth_segmentation_maps)
                
                         
                pred_labels = predicted.detach().cpu().numpy()
                true_labels = labels #labels.detach().cpu().numpy() 
                
                true_labels = np.where(labels == 1, 0, 1)
                

                
                CM = ConfusionMatrix(2, pred_labels, true_labels) 
                IoU = IntersectionOverUnion(CM)
                OA = OverallAccuracy(CM)

                pbar.set_postfix({'Batch': idx, 'Acc': OA, 'IoU':IoU, 'Loss': running_loss/num_samples})
                
                optimizer.step()
            
            #lr_schedule.step()

            if epoch % param_dict['save_iter'] == 0:

                #### Validation ######
                model.eval()
                with torch.no_grad():
                    for idx, batch in enumerate(valid_dataloader):
                        pixel_values = batch["pixel_values"].cuda()
                        mask_labels=[labels.cuda() for labels in batch["mask_labels"]]
                        class_labels=[labels.cuda() for labels in batch["class_labels"]]

                        outputs = model(pixel_values=pixel_values.float(), mask_labels=mask_labels, class_labels = class_labels)
                        val_loss = outputs.loss

                        original_images = batch["original_images"]
                        target_sizes = [(image.shape[0], image.shape[1]) for image in original_images]
                        predicted_segmentation_maps = preprocessor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes) #all predictions maps are 0
                        predicted = torch.stack(predicted_segmentation_maps, dim=0)

                        ground_truth_segmentation_maps = batch["original_segmentation_maps"] #list
                        labels = np.asarray(ground_truth_segmentation_maps)
                
                                
                        pred_labels = predicted.detach().cpu().numpy()
                        true_labels = labels #labels.detach().cpu().numpy()
                        true_labels = np.where(labels == 1, 0, 1)
                        

                        CM = ConfusionMatrix(2, pred_labels, true_labels) 
                        val_IoU = IntersectionOverUnion(CM)
                        OA = OverallAccuracy(CM)
                        F1 = F1Score(CM)

                        val_batch_size = batch["pixel_values"].size(0)
                        
                        val_running_loss += val_loss.item()
                        val_num_samples += val_batch_size

                #### Validation ######
                   
                if epoch >= 20:

                    checkpoint = {
                                "net": model.state_dict(),
                                'optimizer': optimizer.state_dict(),
                                "epoch": epoch
                            }

                    if val_IoU[1] > best_val_acc:
                        name= str(epoch)+'valiou_best.pth'
                    else:
                        name = str(epoch)+'model.pth'
            
                    torch.save(checkpoint, os.path.join(model_dir, name))
                    best_val_acc = val_IoU[1]

                    if early_stopper.early_stop(best_val_acc):
                        print('Early stop break')             
                        break

                
                t_loss= running_loss/num_samples
                v_loss = val_running_loss/val_num_samples
                print(f"Train Loss: {t_loss}, Val IoU: {val_IoU}, Val Loss: {v_loss}")
                cur_log = 'epoch:{}, learning_rate:{}, train_loss:{}, val_loss:{}, val_f1:{}, val_acc:{}, val_miou:{}\n'.format(
                            str(epoch), str(lr), str(t_loss), str(v_loss), str(F1[1]),
                            str(OA),
                            str(val_IoU[1])
                        )
                
                ff.writelines(str(cur_log))



# Hugging Face Inference ðŸ¤—"""

def inference():
    
    if os.path.exists(os.path.join(model_dir, 'test')) is False:
            os.mkdir(os.path.join(model_dir, 'test'))

    df = pd.read_csv('/home/cero_ma/MCV/window_benchmarks/originals/resized/ecp-ref-occ60/train/occ-clases.csv')
    classes = df['name']
    palette = df[['r', 'g', 'b']].values
    id2label = classes.to_dict()
    label2id = {v: k for k, v in id2label.items()}

    
    #feature_extractor = SegformerFeatureExtractor(align=False, reduce_zero_label=False)
    

    checkpoint_path =  os.path.join("/home/cero_ma/MCV/code220419_windows/0401_files/Mask2Former_ecp/150model.pth")#model_dir, model_name)
    #"/home/cero_ma/MCV/code220419_windows/0401_files/test-SegFormer/"

    

    

    state_dict = torch.load(checkpoint_path)['net']
    

    
    model = Mask2FormerForUniversalSegmentation.from_pretrained(pretrained_model_name_or_path = pre_model, 
                                                          ignore_mismatched_sizes=True,
                                                          config = configuration)

    if len(gpu_list) > 1:
        print('gpu>1')  
        model = torch.nn.DataParallel(model, device_ids=gpu_list)

    #model = torch.nn.DataParallel(model, device_ids=[0])
    model.load_state_dict(state_dict)
    print('epoch: ', torch.load(checkpoint_path)['epoch'])
    

    model = model.cuda()

    
    root_dir = "/home/cero_ma/MCV/window_benchmarks/originals/resized/ecp-ref-occ60/"
    #root_dir = "/home/cero_ma/MCV/window_benchmarks/originals/resized/cmp-ref-occ60/"
    image = cv2.imread("/home/cero_ma/MCV/window_benchmarks/originals/resized/ecp-ref-occ60/test/images/monge_31.jpg")
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    mask = cv2.imread("/home/cero_ma/MCV/window_benchmarks/originals/resized/ecp-ref-occ60/test/occ_masks/monge_31.png", cv2.IMREAD_GRAYSCALE)

    #fig, axs = plt.subplots(1, 2, figsize=(20, 10))
    #axs[0].imshow(image)
    #axs[1].imshow(mask)
    #plt.show()

    # prepare the image for the model (aligned resize)
    
    
    g_vars = global_var()
    

    test_dataset = ImageSegmentationDataset(root_dir=root_dir, g_vars= g_vars, inference= True)
    annotations = g_vars.get_var()
    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False,  collate_fn=collate_fn) #TODO: BS=3
    
    test_num = len(annotations)
    label_all = np.zeros((test_num,) + (param_dict['img_size'], param_dict['img_size']), np.uint8)
    predict_all = np.zeros((test_num,) + (param_dict['img_size'], param_dict['img_size']), np.uint8)

    
    batch = next(iter(test_dataloader))
    

    #for k,v in batch:
    #    print(k, v.shape)

    model.eval()
    pbar = tqdm(test_dataloader)
    n = 0

    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)

    meter = AverageMeter()
    optimizer.zero_grad()

    for idx, batch in enumerate(pbar):

        # get the inputs;
    
        pixel_values = batch["pixel_values"].cuda()
        mask_labels=[labels.cuda() for labels in batch["mask_labels"]]
        class_labels=[labels.cuda() for labels in batch["class_labels"]]

        #print(pixel_values.shape)

        
        outputs = model(pixel_values=pixel_values.float())# logits are of shape (batch_size, num_labels, height/4, width/4)

        original_images = batch["original_images"]
        target_sizes = [(image.shape[0], image.shape[1]) for image in original_images]
        predicted_segmentation_maps = preprocessor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)
        predicted = torch.stack(predicted_segmentation_maps, dim=0)

        ground_truth_segmentation_maps = batch["original_segmentation_maps"] #list
        labels = np.asarray(ground_truth_segmentation_maps)
        labels = np.where(labels == 1, 0, 1)
                
                                
                        #pred_labels = predicted.detach().cpu().numpy()
                        #true_labels = labels #labels.detach().cpu().numpy()
        
        

        # Second, apply argmax on the class dimension
        _seg = predicted
        

        for im in range(_seg.shape[0]):
            seg = _seg[im].cpu()
            label = labels[im]


            
            color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3\
            label_all[n] = seg
            
            predict_all[n] = label

            print(palette)


            for label, color in enumerate(palette):
                color_seg[seg == label, :] = color
            # Convert to BGR
            color_seg = color_seg[..., ::-1]
            
            cv2.imwrite(os.path.join(model_dir, 'test', annotations[n]), color_seg)
            n +=1



            # Show image + mask
            #img = np.array(image) * 0.5 + color_seg * 0.5
            #img = img.astype(np.uint8)

            #fig, axs = plt.subplots(1, 2, figsize=(20, 10))
            #axs[0].imshow(img)
            #axs[1].imshow(color_seg)
            #plt.show()
            
             
            #pdb.set_trace()
        
    get_acc_v2(
            label_all, predict_all,
            param_dict['num_class'] + 1 if param_dict['num_class'] == 1 else param_dict['num_class'],
            model_dir)




if len(sys.argv) == 1:
    yaml_file = 'config.yaml'
else:
    yaml_file = sys.argv[1]
param_dict = parse_yaml(yaml_file)

for kv in param_dict.items():
    print(kv)

##############PARAMS"####################################333

WIDTH = 512
HEIGHT = 512
lr = 0.00006 # param_dict['base_lr'] #0.00006 originally used
model_dir = param_dict['save_dir_model'] #'/home/cero_ma/MCV/code220419_windows/0401_files/test-SegFormer/'


model_name = '250valiou_best.pth' # '100model.pth') #

gpu= param_dict['gpu_id'] #'4,5'

os.environ["CUDA_VISIBLE_DEVICES"] = gpu
gpu_list = [i for i in range(len(gpu.split(',')))]
gx = torch.cuda.device_count()
print('useful gpu count is {}'.format(gx))


early_stopper = EarlyStopper(patience=param_dict['stop_pat'], min_delta=param_dict['stop_delta'])

transform = aug.Compose([
    aug.Flip(p=0.5),
    #aug.RandomCrop(width=128, height=128), does not work for this dataset and task
    aug.HorizontalFlip(p=0.5),
    #aug.Normalize(mean=[-0.0711,  0.3911,  0.4208], std=[1.3278, 0.6730, 0.6836]),
    aug.geometric.rotate.Rotate (limit=[-15, 15])
])

transform_train = standard_transforms.Compose([ 
            tr.RandomHorizontalFlip(),
            tr.RandomVerticalFlip(),
            tr.ScaleNRotate(rots=(-15, 15), scales=(0.9, 1.1)),
            tr.FixedResize(param_dict['img_size']),
            tr.Normalize(mean=param_dict['mean'], std=param_dict['std'])
            ])  # data pocessing and data augumentation
transform_val = standard_transforms.Compose([
        tr.FixedResize(param_dict['img_size']),
        tr.Normalize(mean=param_dict['mean'], std=param_dict['std'])
        ])  # data pocessing and data augumentation

configuration =   Mask2FormerConfig(
    activation_function= "relu",
    architectures= [
        "Mask2FormerForUniversalSegmentation"
    ],
    class_weight= 2.0,
    common_stride= 4,
    decoder_layers= 10,
    dice_weight= 5.0,
    dim_feedforward= 2048,
    dropout= 0.0,
    encoder_feedforward_dim= 1024,
    encoder_layers= 6,
    enforce_input_proj= False,
    enforce_input_projection= False,
    feature_size= 256,
    feature_strides= [
        4,
        8,
        16,
        32
    ],
    hidden_dim= 256,
    id2label= {
        2: "background",
        1: "window" #
    },
    ignore_value= 0,
    importance_sample_ratio= 0.75,
    init_std= 0.02,
    init_xavier_std= 1.0,
    label2id= {
        "background": 2,
        "window": 1 #
    },
    mask_feature_size= 256,
    mask_weight= 5.0,
    model_type= "mask2former",
    no_object_weight= 0.1,
    num_attention_heads= 8,
    num_hidden_layers= 10,
    num_queries= 100, #100
    output_auxiliary_logits= None,
    oversample_ratio= 3.0,
    pre_norm= False,
    torch_dtype= "float32",
    train_num_points= 12544,
    transformers_version= None,
    use_auxiliary_loss= True
)     
    


train()
#inference()






