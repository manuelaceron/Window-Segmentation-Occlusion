# -*- coding: utf-8 -*-
"""segformer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_t3KvF3qg4IJfEhTuftFI1GSlscapNgf
"""


from torch.utils.data import Dataset, DataLoader
from transformers import AdamW
import torch, math
from torch import nn
from sklearn.metrics import accuracy_score
from tqdm import tqdm
import os, sys
from PIL import Image
from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor, SegformerConfig, SegformerModel, PretrainedConfig
import pandas as pd
import cv2
import numpy as np
import albumentations as aug
import pdb
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
from tools.metrics import ConfusionMatrix, IntersectionOverUnion, OverallAccuracy, F1Score, get_acc_v2
from tools.parse_config_yaml import parse_yaml
import tools.transform as tr
import torchvision.transforms as standard_transforms
from timm.utils import AverageMeter

def get_input_grad(model, samples):
    outputs = model(samples).logits
    out_size = outputs.size()
    central_point = torch.nn.functional.relu(outputs[:, :, out_size[2] // 2, out_size[3] // 2]).sum()
    grad = torch.autograd.grad(central_point, samples)
    grad = grad[0]
    grad = torch.nn.functional.relu(grad)
    aggregated = grad.sum((0, 1))
    grad_map = aggregated.cpu().numpy()

    return grad_map

def other_visualize_attention(all_attentions, name):
    att_mat = torch.stack(all_attentions[0:3]).squeeze(1) #First layer
    att_mat = torch.mean(att_mat, dim=1)

    residual_att = torch.eye(att_mat.size(1), att_mat.size(2))
    
    aug_att_mat = att_mat.cpu() + residual_att
    
    aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1) #([3, 16384, 256])

    # Recursively multiply the weight matrices
    joint_attentions = torch.zeros(aug_att_mat.size())
    joint_attentions[0] = aug_att_mat[0]

    for n in range(1, aug_att_mat.size(0)):
        joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])
    
    


def visualize_attention(all_attentions, name):
    
    
    for j in range(len(all_attentions)): # For all depths 16
        
        print(j)
        attentions = all_attentions[j][0] #For i depth, and first image
        
        num_heads = attentions.shape[0]
        num_tokens = attentions.shape[1]
        depth = attentions.shape[2]


        #reshape(batch_size, -1, height, width)
        height = int(math.sqrt(num_tokens))
        width = int(math.sqrt(num_tokens))
        
        
        attentions = attentions.permute(0,2,1)

        attentions = attentions.reshape(num_heads, height, width, depth)
        
        gray_att = torch.sum(attentions,3)
        gray_att = gray_att / attentions.shape[3]

        gray_att = attentions

        head_mean = torch.mean(gray_att,0)
        #head_mean = head_mean / gray_att.shape[0]
        

        # Plot the attention maps
        fig = plt.figure(figsize=(30, 50))
        for i in range(1):#num_heads):
                a = fig.add_subplot(5, 4, i+1)
                imgplot = plt.imshow(gray_att[i].data.cpu().numpy())
                a.axis("off")
                #a.set_title(names[i].split('(')[0], fontsize=30)
        
        
        new_name = str(j)+'att_map_'+name
        
        plt.savefig(os.path.join(model_dir, new_name), bbox_inches='tight')
        

    

class EarlyStopper:
    def __init__(self, patience=1, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.min_validation_iou = -1000

    def early_stop(self, validation_iou):
        if validation_iou > self.min_validation_iou:
            self.min_validation_iou = validation_iou
            self.counter = 0
        elif validation_iou < (self.min_validation_iou + self.min_delta):
            self.counter += 1
            if self.counter >= self.patience:
                return True
        return False

class ImageSegmentationDataset(Dataset):
    """Image segmentation dataset."""

    def __init__(self, root_dir, feature_extractor, transforms=None, train=True):
        """
        Args:
            root_dir (string): Root directory of the dataset containing the images + annotations.
            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.
            train (bool): Whether to load "training" or "validation" images + annotations.
        """
        self.root_dir = root_dir
        self.feature_extractor = feature_extractor
        self.train = train
        self.transforms = transforms

        sub_path = "train" if self.train else "val"
        self.img_dir = os.path.join(self.root_dir, sub_path, "images")
        self.ann_dir = os.path.join(self.root_dir, sub_path, "occ_labels") #"occ_masks") #"labels") #")#"occ_masks")
        
        # read images
        image_file_names = []
        for root, dirs, files in os.walk(self.img_dir):
            image_file_names.extend(files)
        self.images = sorted(image_file_names)
        
        # read annotations
        annotation_file_names = []
        for root, dirs, files in os.walk(self.ann_dir):
            annotation_file_names.extend(files)
        self.annotations = sorted(annotation_file_names)

        assert len(self.images) == len(self.annotations), "There must be as many images as there are segmentation maps"

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        
        image = cv2.imread(os.path.join(self.img_dir, self.images[idx]))
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        segmentation_map = cv2.imread(os.path.join(self.ann_dir, self.annotations[idx]))
        segmentation_map = cv2.cvtColor(segmentation_map, cv2.COLOR_BGR2GRAY)
        segmentation_map = np.where(segmentation_map>0, 1, 0)

        sample = {'image': image, 'mask': segmentation_map}

        if self.transforms is not None:
            #augmented = self.transforms(image=image, mask=segmentation_map) #Originally
            augmented = self.transforms(sample)
            
            
            encoded_inputs = self.feature_extractor(augmented['image'], augmented['mask'], return_tensors="pt")
        else:
            encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors="pt")

        for k,v in encoded_inputs.items():
            encoded_inputs[k].squeeze_() # remove batch dimension
        
        return encoded_inputs


class global_var():
    def __init__(self):
        self.var = []

    def set_var(self, var):
        self.var = var

    def get_var(self):
        return self.var

class InferenceImageSegmentationDataset(Dataset):
    """Image segmentation dataset."""

    def __init__(self, root_dir, feature_extractor, g_vars):
        """
        Args:
            root_dir (string): Root directory of the dataset containing the images + annotations.
            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.
            train (bool): Whether to load "training" or "validation" images + annotations.
        """
        self.root_dir = root_dir
        self.feature_extractor = feature_extractor
        

        sub_path = "test" 
        self.img_dir = os.path.join(self.root_dir, sub_path, "images")
        self.ann_dir = os.path.join(self.root_dir, sub_path, "occ_masks")
        
        # read images
        image_file_names = []
        for root, dirs, files in os.walk(self.img_dir):
            image_file_names.extend(files)
        self.images = sorted(image_file_names)
        
        # read annotations
        annotation_file_names = []
        for root, dirs, files in os.walk(self.ann_dir):
            annotation_file_names.extend(files)
        self.annotations = sorted(annotation_file_names)
        

        
        g_vars.set_var(self.annotations)
        
        assert len(self.images) == len(self.annotations), "There must be as many images as there are segmentation maps"

    def __len__(self):
        return len(self.images)
    
    def get_annotations(self):
        return self.annotations

    def __getitem__(self, idx):
        
        image = cv2.imread(os.path.join(self.img_dir, self.images[idx]))
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = cv2.resize(image, (512, 512), interpolation=cv2.INTER_NEAREST)
        
        segmentation_map = cv2.imread(os.path.join(self.ann_dir, self.annotations[idx]))
        segmentation_map = cv2.cvtColor(segmentation_map, cv2.COLOR_BGR2GRAY)
        segmentation_map = cv2.resize(segmentation_map, (512, 512), interpolation=cv2.INTER_NEAREST)
        segmentation_map = np.where(segmentation_map>0, 1, 0)

        encoded_inputs = self.feature_extractor(image, return_tensors="pt").pixel_values.cuda()
        
        encoded_inputs.squeeze_() # remove batch dimension
        
        #for k in encoded_inputs:
        #    encoded_inputs[k].squeeze_() # remove batch dimension
        
       
        return encoded_inputs, segmentation_map

def count_parameters(model):
            return sum(p.numel() for p in model.parameters() if p.requires_grad)


def train():

    start_epoch = 1

    root_dir = '/home/cero_ma/MCV/window_benchmarks/originals/resized/ecp-ref-occ60/'
    feature_extractor = SegformerFeatureExtractor(align=False, reduce_zero_label=False)

    train_dataset = ImageSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor, transforms=transform_train)#transform)
    valid_dataset = ImageSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor, transforms=transform_train)#transform, train=False)

    print("Number of training examples:", len(train_dataset))
    print("Number of validation examples:", len(valid_dataset))


    train_dataloader = DataLoader(train_dataset, batch_size=param_dict['batch_size'], shuffle=True)
    valid_dataloader = DataLoader(valid_dataset, batch_size=param_dict['batch_size'])

    #visible windows torch.tensor([0.60757092, 2.8240482])
    #occlusions [0.57487292, 3.83899089]
    criterion  = nn.CrossEntropyLoss(weight=torch.tensor([0.57487292, 3.83899089])).cuda()
    


    batch = next(iter(train_dataloader))

    for k,v in batch.items():
        print(k, v.shape)

    batch["labels"].shape

    classes = pd.read_csv('/home/cero_ma/MCV/window_benchmarks/originals/resized/ecp-ref-occ60/train/occ-clases.csv')['name']
    id2label = classes.to_dict()
    label2id = {v: k for k, v in id2label.items()}

    
    #model = SegformerForSemanticSegmentation.from_pretrained(pre_model, ignore_mismatched_sizes=True,
    #                                                        num_labels=len(id2label), id2label=id2label, label2id=label2id,
    #                                                        reshape_last_stage=True)

    model = SegformerForSemanticSegmentation.from_pretrained(pretrained_model_name_or_path = pre_model, ignore_mismatched_sizes=True, config = configuration_b2)
    #model = SegformerForSemanticSegmentation(configuration)
    print(model.config)
    
    if len(gpu_list) > 1:
        print('gpu>1')  
        model = torch.nn.DataParallel(model, device_ids=gpu_list)

    optimizer = AdamW(model.parameters(), lr=lr)#, betas=(0.9, 0.999), weight_decay=0.01)
    #lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.8)#50 0.8) 20 0.1

    
    model.cuda()
    
    
    print("Model Initialized!")
    best_val_acc = 0.0
    print('Amount of Parameters: ', count_parameters(model))
    with open(os.path.join(param_dict['save_dir_model'], 'log.txt'), 'w') as ff:
        for epoch in range(start_epoch, param_dict['epoches']+1):  # loop over the dataset multiple times
            print("Epoch:", epoch)
            pbar = tqdm(train_dataloader)
            accuracies = []
            losses = []
            val_accuracies = []
            val_losses = []
            model.train()
            for idx, batch in enumerate(pbar):
                
                # get the inputs;
                pixel_values = batch["pixel_values"].cuda()
                labels = batch["labels"].cuda()

                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                
                outputs = model(pixel_values=pixel_values, labels=labels)
                
                

                # evaluate
                upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode="bilinear", align_corners=False)
                
                loss = criterion(upsampled_logits, labels)
                
                ######

                predicted = upsampled_logits.argmax(dim=1)

                #mask = (labels != 255) # we don't include the background class in the accuracy calculation
                pred_labels = predicted.detach().cpu().numpy()
                true_labels = labels.detach().cpu().numpy()
                
                #
                #accuracy = accuracy_score(pred_labels, true_labels)
                CM = ConfusionMatrix(2, pred_labels, true_labels) 
                IoU = IntersectionOverUnion(CM)
                OA = OverallAccuracy(CM)

                
                #loss = outputs.loss
                #accuracies.append(accuracy)
                
                losses.append(loss.sum())
                
                pbar.set_postfix({'Batch': idx, 'Acc': OA, 'IoU':IoU, 'Loss': sum(losses)/len(losses)})
                #pbar.set_postfix({'Batch': idx, 'Loss': sum(losses)/len(losses)})

                # backward + optimize
                loss.sum().backward()
                optimizer.step()
            
            #lr_schedule.step()

            if epoch % param_dict['save_iter'] == 0:

                #### Validation ######
                model.eval()
                with torch.no_grad():
                    for idx, batch in enumerate(valid_dataloader):
                        pixel_values = batch["pixel_values"].cuda()
                        labels = batch["labels"].cuda()

                        outputs = model(pixel_values=pixel_values, labels=labels)
                        upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode="bilinear", align_corners=False)
                        predicted = upsampled_logits.argmax(dim=1)

                        #mask = (labels != 255) # we don't include the background class in the accuracy calculation
                        pred_labels = predicted.detach().cpu().numpy()
                        true_labels = labels.detach().cpu().numpy()

                        CM = ConfusionMatrix(2, pred_labels, true_labels) 
                        val_IoU = IntersectionOverUnion(CM)
                        OA = OverallAccuracy(CM)
                        F1 = F1Score(CM)

                        val_loss = outputs.loss
                        val_losses.append(val_loss.sum())

                #### Validation ######
                   
                if epoch >= 20:

                    checkpoint = {
                                "net": model.state_dict(),
                                'optimizer': optimizer.state_dict(),
                                "epoch": epoch
                            }

                    if val_IoU[1] > best_val_acc:
                        name= str(epoch)+'valiou_best.pth'
                    else:
                        name = str(epoch)+'model.pth'
            
                    torch.save(checkpoint, os.path.join(model_dir, name))
                    best_val_acc = val_IoU[1]

                    if early_stopper.early_stop(best_val_acc):
                        print('Early stop break')             
                        break


                t_loss= sum(losses)/len(losses)
                v_loss = sum(val_losses)/len(val_losses)
                print(f"Train Loss: {t_loss}, Val IoU: {val_IoU}, Val Loss: {v_loss}")
                cur_log = 'epoch:{}, learning_rate:{}, train_loss:{}, val_loss:{}, val_f1:{}, val_acc:{}, val_miou:{}\n'.format(
                            str(epoch), str(lr), str(t_loss.item()), str(v_loss.item()), str(F1[1]),
                            str(OA),
                            str(val_IoU[1])
                        )
                
                ff.writelines(str(cur_log))



# Hugging Face Inference ðŸ¤—"""

def inference():
    
    if os.path.exists(os.path.join(model_dir, 'test')) is False:
            os.mkdir(os.path.join(model_dir, 'test'))

    df = pd.read_csv('/home/cero_ma/MCV/window_benchmarks/originals/resized/ecp-ref-occ60/train/occ-clases.csv')
    classes = df['name']
    palette = df[['r', 'g', 'b']].values
    id2label = classes.to_dict()
    label2id = {v: k for k, v in id2label.items()}

    
    #feature_extractor = SegformerFeatureExtractor(align=False, reduce_zero_label=False)
    

    checkpoint_path =  os.path.join("/home/cero_ma/MCV/code220419_windows/0401_files/occ: SegFormer/SegFormer_ecp_occ_b2_loss_aug_lr0_00006_preCityS/250valiou_best.pth")#model_dir, model_name)
    #"/home/cero_ma/MCV/code220419_windows/0401_files/test-SegFormer/"

    

    

    state_dict = torch.load(checkpoint_path)['net']
    

    
    #model = SegformerForSemanticSegmentation.from_pretrained(pre_model, ignore_mismatched_sizes=True,
    #                                                        num_labels=len(id2label), id2label=id2label, label2id=label2id,
    #                                                        reshape_last_stage=True)

    model = SegformerForSemanticSegmentation.from_pretrained(pretrained_model_name_or_path = pre_model, ignore_mismatched_sizes=True, config = configuration_b2)

    if len(gpu_list) > 1:
        print('gpu>1')  
        model = torch.nn.DataParallel(model, device_ids=gpu_list)

    model = torch.nn.DataParallel(model, device_ids=[0])
    model.load_state_dict(state_dict)
    print('epoch: ', torch.load(checkpoint_path)['epoch'])
    

    model = model.cuda()

    
    root_dir = "/home/cero_ma/MCV/window_benchmarks/originals/resized/ecp-ref-occ60/"
    image = cv2.imread("/home/cero_ma/MCV/window_benchmarks/originals/resized/ecp-ref-occ60/test/images/monge_31.jpg")
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    mask = cv2.imread("/home/cero_ma/MCV/window_benchmarks/originals/resized/ecp-ref-occ60/test/occ_masks/monge_31.png", cv2.IMREAD_GRAYSCALE)

    #fig, axs = plt.subplots(1, 2, figsize=(20, 10))
    #axs[0].imshow(image)
    #axs[1].imshow(mask)
    #plt.show()

    # prepare the image for the model (aligned resize)
    
    #pixel_values = feature_extractor_inference(image, return_tensors="pt").pixel_values.cuda()
    feature_extractor_inference = SegformerFeatureExtractor(do_random_crop=False, do_pad=False)
    g_vars = global_var()
    

    test_dataset = InferenceImageSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor_inference, g_vars= g_vars)
    annotations = g_vars.get_var()
    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False) #TODO: BS=3
    
    test_num = len(annotations)
    label_all = np.zeros((test_num,) + (param_dict['img_size'], param_dict['img_size']), np.uint8)
    predict_all = np.zeros((test_num,) + (param_dict['img_size'], param_dict['img_size']), np.uint8)

    
    batch = next(iter(test_dataloader))
    

    #for k,v in batch:
    #    print(k, v.shape)

    model.eval()
    pbar = tqdm(test_dataloader)
    n = 0

    optimizer = AdamW(model.parameters(), lr=lr)

    meter = AverageMeter()
    optimizer.zero_grad()

    for idx, batch in enumerate(pbar):

        # get the inputs;
    
        pixel_values = batch[0].cuda()
        labels = batch[1].cuda()

        #print(pixel_values.shape)
        
        
        outputs = model(pixel_values=pixel_values, output_attentions=True)# logits are of shape (batch_size, num_labels, height/4, width/4)

        
        

        #ERF(meter, pixel_values, optimizer, model, outputs)
        ###########################################
        
        if meter.count == 20:
            
            #np.save(os.path.join(model_dir,'ERF.png'), meter.avg)
            print('uncomment')
            #exit()

        samples = pixel_values.cuda(non_blocking=True)
        samples.requires_grad = True
        optimizer.zero_grad()
        contribution_scores = get_input_grad(model, samples)
        
        if annotations[n] == 'monge_61.png':
            print('Here')
            meter.update(contribution_scores)
            np.save(os.path.join(model_dir,'ERF_61'), meter.avg)

        if np.isnan(np.sum(contribution_scores)):
            print('got NAN, next image')
            continue
        else:
            print('accumulate')
            #meter.update(contribution_scores)
        ###########################################

        logits = outputs.logits.cpu()
        print('logits.shape: ', logits.shape)
        

        # First, rescale logits to original image size
        upsampled_logits = nn.functional.interpolate(logits,
                        size=image.shape[:-1], # (height, width)
                        mode='bilinear',
                        align_corners=False)
        
        

        # Second, apply argmax on the class dimension
        _seg = upsampled_logits.argmax(dim=1)

        for im in range(batch[0].shape[0]):
            seg = _seg[im]
            label = labels[im]


            
            color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3\
            label_all[n] = seg
            
            predict_all[n] = label.cpu()

            

            for label, color in enumerate(palette):
                color_seg[seg == label, :] = color
            # Convert to BGR
            color_seg = color_seg[..., ::-1]
            
            cv2.imwrite(os.path.join(model_dir, 'test', annotations[n]), color_seg)
            
            #if annotations[n] == 'monge_86.png':
                #other_visualize_attention(outputs.attentions, annotations[n])
                #visualize_attention(outputs.attentions, annotations[n])
                
            

            n +=1



            # Show image + mask
            #img = np.array(image) * 0.5 + color_seg * 0.5
            #img = img.astype(np.uint8)

            #fig, axs = plt.subplots(1, 2, figsize=(20, 10))
            #axs[0].imshow(img)
            #axs[1].imshow(color_seg)
            #plt.show()
            
             
            #pdb.set_trace()
        
    get_acc_v2(
            label_all, predict_all,
            param_dict['num_class'] + 1 if param_dict['num_class'] == 1 else param_dict['num_class'],
            model_dir)

    	

    #torch.save(outputs.attentions, os.path.join(model_dir, 'segFormerOccAttention.t'))




if len(sys.argv) == 1:
    yaml_file = 'config.yaml'
else:
    yaml_file = sys.argv[1]
param_dict = parse_yaml(yaml_file)

for kv in param_dict.items():
    print(kv)

##############PARAMS"####################################333

WIDTH = 512
HEIGHT = 512
lr = 0.00006 # param_dict['base_lr'] #0.00006 originally used
model_dir = param_dict['save_dir_model'] #'/home/cero_ma/MCV/code220419_windows/0401_files/test-SegFormer/'

pre_model = "nvidia/segformer-b2-finetuned-cityscapes-1024-1024" #"nvidia/mit-b2" #" # #"nvidia/segformer-b1-finetuned-cityscapes-1024-1024" # "nvidia/mit-b2"
model_name = '250valiou_best.pth' # '100model.pth') #

gpu= param_dict['gpu_id'] #'4,5'

os.environ["CUDA_VISIBLE_DEVICES"] = gpu
gpu_list = [i for i in range(len(gpu.split(',')))]
gx = torch.cuda.device_count()
print('useful gpu count is {}'.format(gx))


early_stopper = EarlyStopper(patience=param_dict['stop_pat'], min_delta=param_dict['stop_delta'])

transform = aug.Compose([
    aug.Flip(p=0.5),
    #aug.RandomCrop(width=128, height=128), does not work for this dataset and task
    aug.HorizontalFlip(p=0.5),
    aug.Normalize(mean=param_dict['mean'], std=param_dict['std']),
    aug.geometric.rotate.Rotate (limit=[-15, 15])
])

transform_train = standard_transforms.Compose([ 
            tr.RandomHorizontalFlip(),
            tr.RandomVerticalFlip(),
            tr.ScaleNRotate(rots=(-15, 15), scales=(0.9, 1.1)),
            tr.FixedResize(param_dict['img_size'])
            #tr.Normalize(mean=param_dict['mean'], std=param_dict['std'])
            ])  # data pocessing and data augumentation
transform_val = standard_transforms.Compose([
        tr.FixedResize(param_dict['img_size']),
        tr.Normalize(mean=param_dict['mean'], std=param_dict['std'])
        ])  # data pocessing and data augumentation

configuration_b1 = PretrainedConfig( #SegformerConfig(
            #_name_or_path = "nvidia/mit-b2", #"nvidia/segformer-b2-finetuned-cityscapes-1024-1024",
            architectures = ["SegformerForSemanticSegmentation"],
            attention_probs_dropout_prob = 0.0,
            classifier_dropout_prob = 0.1,
            decoder_hidden_size = 256, 
            depths = [2, 2, 2, 2], # The number of layers in each encoder block. Paper says 3,3,6,3
            downsampling_rates = [1, 4, 8, 16],
            drop_path_rate = 0.1,
            hidden_act = 'gelu',
            hidden_dropout_prob = 0.0,
            hidden_sizes = [64, 128, 320, 512], # Channel number of output stage (same b1-b5)
            id2label = { "0": "background", "1": "occlusion"},
            image_size = 224,
            initializer_range = 0.02,
            label2id = { "background": 0, "occlusion": 1 },
            layer_norm_eps = 1e-06,
            mlp_ratios = [4, 4, 4, 4],
            model_type = "segformer",
            num_attention_heads = [1, 2, 5, 8], # Self-attention heads in stage (same b0-b5)
            num_channels = 3, 
            num_encoder_blocks = 4, 
            patch_sizes = [7, 3, 3, 3], # Patch size (same b0-b5)
            reshape_last_stage = True,
            semantic_loss_ignore_index = 255,
            sr_ratios = [8, 4, 2, 1], # Reduction ratio of the efficient Self-Att (same b0-b5)
            strides = [4, 2, 2, 2], # Stride between 2 adjacent patches (same b0-b5)
            ignore_mismatched_sizes=True,
            torch_dtype = "float32",
            transformers_version = "4.18.0"
            ) 

configuration_b2 = PretrainedConfig( #SegformerConfig(
            #_name_or_path = "nvidia/mit-b2", #"nvidia/segformer-b2-finetuned-cityscapes-1024-1024",
            architectures = ["SegformerForSemanticSegmentation"],
            attention_probs_dropout_prob = 0.0,
            classifier_dropout_prob = 0.1,
            decoder_hidden_size = 768, #1024, # 256, #768, 
            depths = [3, 4, 6, 3], #[3, 4, 18, 3], #[3, 4, 6, 3], #[3, 4, 6, 3], # #, #, #[2, 2, 2, 2], # # The number of layers in each encoder block. Paper says 3,3,6,3
            downsampling_rates = [1, 4, 8, 16],
            drop_path_rate = 0.1,
            hidden_act = 'gelu',
            hidden_dropout_prob = 0.0,
            hidden_sizes = [64, 128, 320, 512], #  #128, 256, 640, 1024], #, # #  #[32, 64, 160, 256], Channel number of output stage (same b1-b5)
            id2label = { "0": "background", "1": "occlusion"},
            image_size = 224, # I think is not used...
            initializer_range = 0.02,
            label2id = { "background": 0, "occlusion": 1 },
            layer_norm_eps = 1e-06,
            mlp_ratios = [4, 4, 4, 4], #[2, 2, 2, 2], #[4, 4, 4, 4],  #[8, 8, 4, 4], #
            model_type = "segformer",
            num_attention_heads = [1, 2, 5, 8], # Self-attention heads in stage (same b0-b5)
            num_channels = 3, 
            num_encoder_blocks = 4, 
            patch_sizes = [7, 3, 3, 3], #[7, 3, 3, 3], # Patch size (same b0-b5)
            reshape_last_stage = True,
            semantic_loss_ignore_index = 255,
            sr_ratios = [8, 4, 2, 1], # Reduction ratio of the efficient Self-Att (same b0-b5)
            strides = [4, 2, 2, 2], #[4, 2, 2, 2], # Stride between 2 adjacent patches (same b0-b5)
            ignore_mismatched_sizes=True,
            torch_dtype = "float32",
            transformers_version = "4.18.0"
            ) 

configuration_b2_shallow = PretrainedConfig( #SegformerConfig(
            #_name_or_path = "nvidia/mit-b2", #"nvidia/segformer-b2-finetuned-cityscapes-1024-1024",
            architectures = ["SegformerForSemanticSegmentation"],
            attention_probs_dropout_prob = 0.0,
            classifier_dropout_prob = 0.1,
            decoder_hidden_size = 768, #1024, # 256, #768, 
            depths = [3, 4, 6], #[3, 4, 18, 3], #[3, 4, 6, 3], #[3, 4, 6, 3], # #, #, #[2, 2, 2, 2], # # The number of layers in each encoder block. Paper says 3,3,6,3
            downsampling_rates = [1, 4, 8],
            drop_path_rate = 0.1,
            hidden_act = 'gelu',
            hidden_dropout_prob = 0.0,
            hidden_sizes = [64, 128, 320], #  #128, 256, 640, 1024], #, # #  #[32, 64, 160, 256], Channel number of output stage (same b1-b5)
            id2label = { "0": "background", "1": "occlusion"},
            image_size = 224, # I think is not used...
            initializer_range = 0.02,
            label2id = { "background": 0, "occlusion": 1 },
            layer_norm_eps = 1e-06,
            mlp_ratios = [4, 4, 4], #[2, 2, 2, 2], #[4, 4, 4, 4],  #[8, 8, 4, 4], #
            model_type = "segformer",
            num_attention_heads = [1, 2, 5], # Self-attention heads in stage (same b0-b5)
            num_channels = 3, 
            num_encoder_blocks = 3, 
            patch_sizes = [7, 3, 3], #[7, 3, 3, 3], # Patch size (same b0-b5)
            reshape_last_stage = True,
            semantic_loss_ignore_index = 255,
            sr_ratios = [8, 4, 2], # Reduction ratio of the efficient Self-Att (same b0-b5)
            strides = [4, 2, 2], #[4, 2, 2, 2], # Stride between 2 adjacent patches (same b0-b5)
            ignore_mismatched_sizes=True,
            torch_dtype = "float32",
            transformers_version = "4.18.0"
            ) 

configuration_b3 = PretrainedConfig( #SegformerConfig(
            #_name_or_path = "nvidia/mit-b2", #"nvidia/segformer-b2-finetuned-cityscapes-1024-1024",
            architectures = ["SegformerForSemanticSegmentation"],
            attention_probs_dropout_prob = 0.0,
            classifier_dropout_prob = 0.1,
            decoder_hidden_size = 768, 
            depths = [3, 4, 18, 3], # The number of layers in each encoder block. Paper says 3,3,18,3
            downsampling_rates = [1, 4, 8, 16],
            drop_path_rate = 0.1,
            hidden_act = 'gelu',
            hidden_dropout_prob = 0.0,
            hidden_sizes = [64, 128, 320, 512], # Channel number of output stage (same b1-b5)
            id2label = { "0": "background", "1": "occlusion"},
            image_size = 224,
            initializer_range = 0.02,
            label2id = { "background": 0, "occlusion": 1 },
            layer_norm_eps = 1e-06,
            mlp_ratios = [4, 4, 4, 4],
            model_type = "segformer",
            num_attention_heads = [1, 2, 5, 8], # Self-attention heads in stage (same b0-b5)
            num_channels = 3, 
            num_encoder_blocks = 4, 
            patch_sizes = [7, 3, 3, 3], # Patch size (same b0-b5)
            reshape_last_stage = True,
            semantic_loss_ignore_index = 255,
            sr_ratios = [8, 4, 2, 1], # Reduction ratio of the efficient Self-Att (same b0-b5)
            strides = [4, 2, 2, 2], # Stride between 2 adjacent patches (same b0-b5)
            ignore_mismatched_sizes=True,
            torch_dtype = "float32",
            transformers_version = "4.18.0"
            )         

train()
#inference()






