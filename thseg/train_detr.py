# -*- coding: utf-8 -*-
"""segformer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_t3KvF3qg4IJfEhTuftFI1GSlscapNgf
"""


from torch.utils.data import Dataset, DataLoader
import torch
from torch import nn
from sklearn.metrics import accuracy_score
from tqdm import tqdm
import os, sys
from PIL import Image
from transformers import PretrainedConfig, AutoImageProcessor, DetrForSegmentation, DetrConfig, SegformerFeatureExtractor
import pandas as pd
import cv2
import numpy as np
import albumentations as aug
import pdb
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
from tools.metrics import ConfusionMatrix, IntersectionOverUnion, OverallAccuracy, F1Score, get_acc_v2
from tools.parse_config_yaml import parse_yaml
import tools.transform as tr
import torchvision.transforms as standard_transforms
from timm.utils import AverageMeter
from tools.losses import get_loss




def get_input_grad(model, samples):
    outputs = model(samples).logits
    out_size = outputs.size()
    central_point = torch.nn.functional.relu(outputs[:, :, out_size[2] // 2, out_size[3] // 2]).sum()
    grad = torch.autograd.grad(central_point, samples)
    grad = grad[0]
    grad = torch.nn.functional.relu(grad)
    aggregated = grad.sum((0, 1))
    grad_map = aggregated.cpu().numpy()

    return grad_map

    
def visualize_attention(attentions):
    num_heads = attentions.shape[0]
    num_tokens = attentions.shape[1]


    #reshape(batch_size, -1, height, width)
    
    attentions = attentions.view(num_heads, num_tokens, num_tokens)

    # Normalize the attention weights across the sequence
    attentions = torch.softmax(attentions, dim=0)
    #pdb.set_trace()
    #attentions = attentions.reshape(num_heads, 512, 512)

    # Create subplots for each head
    fig, axs = plt.subplots(num_heads, 1, figsize=(8, 4*num_heads))
    if num_heads == 1:
        axs = [axs]  # Handle the case of a single head

    # Plot the attention maps
    for head in range(num_heads):
        ax = axs[head]
        ax.imshow(attentions[head].detach().cpu().numpy(), cmap='inferno', interpolation='nearest')

        # Customize the plot
        #ax.set_xticks(range(num_tokens))
        #ax.set_yticks(range(num_tokens))
        #ax.set_xticklabels(range(1, num_tokens+1))
        #ax.set_yticklabels(range(1, num_tokens+1))
        ax.set_xlabel('Input Tokens')
        ax.set_ylabel('Output Tokens')
        ax.set_title(f'Head {head+1} Attention')

    # Adjust spacing between subplots
    plt.tight_layout()

    # Show the plot
    plt.show()

class EarlyStopper:
    def __init__(self, patience=1, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.min_validation_iou = -1000

    def early_stop(self, validation_iou):
        if validation_iou > self.min_validation_iou:
            self.min_validation_iou = validation_iou
            self.counter = 0
        elif validation_iou < (self.min_validation_iou + self.min_delta):
            self.counter += 1
            if self.counter >= self.patience:
                return True
        return False

class original_ImageSegmentationDataset(Dataset):
    """Image segmentation dataset."""

    def __init__(self, root_dir, feature_extractor, transforms=None, train=True):
        """
        Args:
            root_dir (string): Root directory of the dataset containing the images + annotations.
            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.
            train (bool): Whether to load "training" or "validation" images + annotations.
        """
        self.root_dir = root_dir
        self.feature_extractor = feature_extractor
        self.train = train
        self.transforms = transforms

        sub_path = "train" if self.train else "val"
        self.img_dir = os.path.join(self.root_dir, sub_path, "images")
        self.ann_dir = os.path.join(self.root_dir, sub_path, "occ_labels") #"occ_masks") #"labels") #")#"occ_masks")
        
        # read images
        image_file_names = []
        for root, dirs, files in os.walk(self.img_dir):
            image_file_names.extend(files)
        self.images = sorted(image_file_names)
        
        # read annotations
        annotation_file_names = []
        for root, dirs, files in os.walk(self.ann_dir):
            annotation_file_names.extend(files)
        self.annotations = sorted(annotation_file_names)

        assert len(self.images) == len(self.annotations), "There must be as many images as there are segmentation maps"

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        
        image = cv2.imread(os.path.join(self.img_dir, self.images[idx]))
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        segmentation_map = cv2.imread(os.path.join(self.ann_dir, self.annotations[idx]))
        segmentation_map = cv2.cvtColor(segmentation_map, cv2.COLOR_BGR2GRAY)
        segmentation_map = np.where(segmentation_map>0, 1, 0)

        sample = {'image': image, 'mask': segmentation_map}

        if self.transforms is not None:
            augmented = self.transforms(image=image, mask=segmentation_map) #Originally
            #augmented = self.transforms(sample)
            
            pdb.set_trace()
            encoded_inputs = self.feature_extractor(augmented['image'], augmented['mask'], return_tensors="pt")
        else:
            encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors="pt")

        for k,v in encoded_inputs.items():
            encoded_inputs[k].squeeze_() # remove batch dimension
        
        
        return encoded_inputs

class ImageSegmentationDataset(Dataset):
    """Image segmentation dataset."""

    def __init__(self, root_dir, transforms=None, train=True, inference= False, g_vars =None):
        """
        Args:
            root_dir (string): Root directory of the dataset containing the images + annotations.
            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.
            train (bool): Whether to load "training" or "validation" images + annotations.
        """
        self.root_dir = root_dir
        self.train = train
        self.transforms = transforms

        if not inference:
            sub_path = "train" if self.train else "val"
        else:
            sub_path = "test"

        self.img_dir = os.path.join(self.root_dir, sub_path, "images")
        self.ann_dir = os.path.join(self.root_dir, sub_path, "labels") #"occ_masks") #"labels") #")#"occ_masks")
        
        # read images
        image_file_names = []
        for root, dirs, files in os.walk(self.img_dir):
            image_file_names.extend(files)
        self.images = sorted(image_file_names)
        
        # read annotations
        annotation_file_names = []
        for root, dirs, files in os.walk(self.ann_dir):
            annotation_file_names.extend(files)
        self.annotations = sorted(annotation_file_names)

        if g_vars is not None:
            g_vars.set_var(self.annotations)

        assert len(self.images) == len(self.annotations), "There must be as many images as there are segmentation maps"

    def __len__(self):
        return len(self.images)
    
    def get_annotations(self):
        return self.annotations

    def __getitem__(self, idx):
        
        image = cv2.imread(os.path.join(self.img_dir, self.images[idx]))
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        segmentation_map = cv2.imread(os.path.join(self.ann_dir, self.annotations[idx]))
        segmentation_map = cv2.cvtColor(segmentation_map, cv2.COLOR_BGR2GRAY)
        segmentation_map = np.where(segmentation_map>0, 1, 0) 
        #segmentation_map = segmentation_map[np.where((segmentation_map == [0, 0, 0]).all(axis=2))] = [255, 0, 0]

        sample = {'image': image, 'mask': segmentation_map}

        if self.transforms is not None:
            augmented = self.transforms(image=image, mask=segmentation_map) #Originally
            tr_image, tr_segmentation_map = augmented['image'], augmented['mask']
        else:
            tr_image= image
            tr_segmentation_map =segmentation_map

        
        # convert to C, H, W
        tr_image = image.transpose(2,0,1)
        
        return tr_image, tr_segmentation_map, image, segmentation_map


class global_var():
    def __init__(self):
        self.var = []

    def set_var(self, var):
        self.var = var

    def get_var(self):
        return self.var



def count_parameters(model):
            return sum(p.numel() for p in model.parameters() if p.requires_grad)

pre_model = "facebook/detr-resnet-50-panoptic" 
#preprocessor = Mask2FormerImageProcessor(ignore_index=0, reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False) #
preprocessor = AutoImageProcessor.from_pretrained(pre_model, ignore_index=0, reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)
feature_extractor = SegformerFeatureExtractor(align=False, reduce_zero_label=False)
def collate_fn(batch):
    # Create a preprocessor
   
    

    inputs = list(zip(*batch))
    images = inputs[0]
    segmentation_maps = inputs[1]
    pdb.set_trace()
    # this function pads the inputs to the same size,
    # and creates a pixel mask
    # actually padding isn't required here since we are cropping
    batch = preprocessor(
        images,
        segmentation_maps=segmentation_maps,
        return_tensors="pt",
    )

    
    labels = np.stack(segmentation_maps)
    batch["ori_labels"] = labels #torch.from_numpy(labels)
    batch["original_images"] = inputs[2]
    batch["original_segmentation_maps"] = inputs[3]
    
    #batch[0] pixel_values
    #batch[1] pixel_mask
    
    encoded_inputs = feature_extractor(images, segmentation_maps, return_tensors="pt")
    for k,v in encoded_inputs.items():
            encoded_inputs[k].squeeze_() # remove batch dimension
    
    batch["labels"] = encoded_inputs['labels']
    return batch
    

def train():
    
    



    start_epoch = 1

    #root_dir = '/home/cero_ma/MCV/window_benchmarks/originals/resized/cmp-occluded60/'
    root_dir = "/home/cero_ma/MCV/window_benchmarks/originals/resized/ecp-ref-occ60/"

    

    train_dataset = ImageSegmentationDataset(root_dir=root_dir, transforms=transform)
    valid_dataset = ImageSegmentationDataset(root_dir=root_dir, transforms=transform, train=False)

    print("Number of training examples:", len(train_dataset))
    print("Number of validation examples:", len(valid_dataset))


    train_dataloader = DataLoader(train_dataset, batch_size=param_dict['batch_size'], shuffle=True, collate_fn=collate_fn)
    valid_dataloader = DataLoader(valid_dataset, batch_size=param_dict['batch_size'], collate_fn=collate_fn)

    #visible windows torch.tensor([0.60757092, 2.8240482])
    #occlusions [0.57487292, 3.83899089]
    #criterion  = nn.CrossEntropyLoss(weight=torch.tensor([0.60757092, 2.8240482])).cuda()
    #criterion = torch.nn.BCEWithLogitsLoss().cuda()
    #criterion = torch.nn.BCELoss().cuda()
    criterion = get_loss("bce_log", torch.tensor([0.63147511, 2.40150057]))
    


    batch = next(iter(train_dataloader))
    
    for k,v in batch.items():
        if isinstance(v, torch.Tensor):
            print(k,v.shape)
        else:
            print(k,v[0].shape)
    



    #classes = pd.read_csv('/home/cero_ma/MCV/window_benchmarks/originals/resized/ecp-ref-occ60/train/occ-clases.csv')['name']
    #id2label = classes.to_dict()
    #label2id = {v: k for k, v in id2label.items()}

    
    #id2label = {1: "occlusion"}
    #label2id = {"occlusion": 1 }

    #model = SegformerForSemanticSegmentation.from_pretrained(pretrained_model_name_or_path = pre_model, ignore_mismatched_sizes=True, config = configuration_b2)
    
    

    model = DetrForSegmentation.from_pretrained(pre_model, config= config, ignore_mismatched_sizes=True)        

    print(model.config)
    
    if len(gpu_list) > 1:
        print('gpu>1')  
        model = torch.nn.DataParallel(model, device_ids=gpu_list)

    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)#, betas=(0.9, 0.999), weight_decay=0.01)
    #lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.8)#50 0.8) 20 0.1

    
    model.cuda()
    
    
    print("Model Initialized!")
    best_val_acc = 0.0
    running_loss = 0.0
    num_samples = 0
    val_running_loss = 0.0
    val_num_samples = 0
    print('Amount of Parameters: ', count_parameters(model))
    with open(os.path.join(param_dict['save_dir_model'], 'log.txt'), 'w') as ff:
        for epoch in range(start_epoch, param_dict['epoches']+1):  # loop over the dataset multiple times
            print("Epoch:", epoch)
            pbar = tqdm(train_dataloader)
            accuracies = []
            losses = []
            val_accuracies = []
            val_losses = []
            model.train()
            for idx, batch in enumerate(pbar):

                
                
                # get the inputs;
                pixel_values = batch["pixel_values"].cuda()
                pixel_mask = batch['pixel_mask'].cuda()
                labels = batch["labels"].cuda()
                ori_labels = batch["ori_labels"]
                
                

                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                #visual_mask = (batch["mask_labels"][0][0].bool().numpy() * 255).astype(np.uint8) # (1,512,512) for each image, there are N mask labels (N:classes)
                #cv2.imshow("f", visual_mask)

                #pixel_mask = (batch["pixel_mask"][0].bool().numpy() * 255).astype(np.uint8) #Original GT?
                #cv2.imshow("d", pixel_mask)
                #cv2.waitKey(0)
                #cv2.destroyAllWindows()

                
                
                
                

                outputs = model(pixel_values=pixel_values.float())#, pixel_mask = pixel_mask) #labels= labels, 
                
                
                #predicted_maps = preprocessor.post_process_semantic_segmentation(outputs, target_sizes=None)
                upsampled_logits = nn.functional.interpolate(outputs, size=labels.shape[-2:], mode="bilinear", align_corners=False)

                loss = criterion(upsampled_logits, labels)

                
                pred = upsampled_logits.cpu().data.numpy()[:, 0, :, :]
                pred[pred >= 0.5] = 1
                pred[pred < 0.5] = 0
                pred_labels = pred.astype(np.uint32)
                true_labels = ori_labels
                
                
                #pred_labels = upsampled_logits.detach().cpu().numpy()
                #true_labels = labels.detach().cpu().numpy()

                CM = ConfusionMatrix(2, pred_labels, true_labels) 
                IoU = IntersectionOverUnion(CM)
                OA = OverallAccuracy(CM)
                
                losses.append(loss.sum())
                
                #pbar.set_postfix({'Batch': idx, 'Loss': sum(losses)/len(losses)})
                pbar.set_postfix({'Batch': idx, 'Acc': OA, 'IoU':IoU, 'Loss': sum(losses)/len(losses)})
                
                loss.sum().backward()
                optimizer.step()

                
            
            #lr_schedule.step()

            if epoch % param_dict['save_iter'] == 0:

                #### Validation ######
                model.eval()
                with torch.no_grad():
                    for idx, batch in enumerate(valid_dataloader):
                        pixel_values = batch["pixel_values"].cuda()
                        pixel_mask = batch['pixel_mask'].cuda()
                        labels = batch["labels"].cuda()
                        ori_labels = batch["ori_labels"]
                        
                        outputs = model(pixel_values=pixel_values.float())#, pixel_mask = pixel_mask) #labels= labels, 
            
                        upsampled_logits = nn.functional.interpolate(outputs, size=labels.shape[-2:], mode="bilinear", align_corners=False)
                        
                        val_loss = criterion(upsampled_logits, labels)

                        pred = upsampled_logits.cpu().data.numpy()[:, 0, :, :]
                        pred[pred >= 0.5] = 1
                        pred[pred < 0.5] = 0
                        pred_labels = pred.astype(np.uint32)
                        true_labels = ori_labels
                        

                        CM = ConfusionMatrix(2, pred_labels, true_labels) 
                        val_IoU = IntersectionOverUnion(CM)
                        OA = OverallAccuracy(CM)
                        F1 = F1Score(CM)

                        val_losses.append(val_loss.sum())

                        

                #### Validation ######
                   
                if epoch >= 20:

                    checkpoint = {
                                "net": model.state_dict(),
                                'optimizer': optimizer.state_dict(),
                                "epoch": epoch
                            }

                    if val_IoU[1] > best_val_acc:
                        name= str(epoch)+'valiou_best.pth'
                    else:
                        name = str(epoch)+'model.pth'
            
                    torch.save(checkpoint, os.path.join(model_dir, name))
                    best_val_acc = val_IoU[1]

                    if early_stopper.early_stop(best_val_acc):
                        print('Early stop break')             
                        break

                
                t_loss= sum(losses)/len(losses)
                v_loss = sum(val_losses)/len(val_losses)
                print(f"Train Loss: {t_loss}, Val IoU: {val_IoU}, Val Loss: {v_loss}")
                cur_log = 'epoch:{}, learning_rate:{}, train_loss:{}, val_loss:{}, val_f1:{}, val_acc:{}, val_miou:{}\n'.format(
                            str(epoch), str(lr), str(t_loss), str(v_loss), str(F1[1]),
                            str(OA),
                            str(val_IoU[1])
                        )
                
                ff.writelines(str(cur_log))



# Hugging Face Inference ðŸ¤—"""

def inference():
    
    if os.path.exists(os.path.join(model_dir, 'test')) is False:
            os.mkdir(os.path.join(model_dir, 'test'))

    df = pd.read_csv('/home/cero_ma/MCV/window_benchmarks/originals/resized/ecp-ref-occ60/train/occ-clases.csv')
    classes = df['name']
    palette = df[['r', 'g', 'b']].values
    id2label = classes.to_dict()
    label2id = {v: k for k, v in id2label.items()}

    
    #feature_extractor = SegformerFeatureExtractor(align=False, reduce_zero_label=False)
    

    checkpoint_path =  os.path.join("/home/cero_ma/MCV/code220419_windows/0401_files/Mask2Former_ecp/150model.pth")#model_dir, model_name)
    #"/home/cero_ma/MCV/code220419_windows/0401_files/test-SegFormer/"

    

    

    state_dict = torch.load(checkpoint_path)['net']
    

    
    model = Mask2FormerForUniversalSegmentation.from_pretrained(pretrained_model_name_or_path = pre_model, 
                                                          ignore_mismatched_sizes=True,
                                                          config = configuration)

    if len(gpu_list) > 1:
        print('gpu>1')  
        model = torch.nn.DataParallel(model, device_ids=gpu_list)

    #model = torch.nn.DataParallel(model, device_ids=[0])
    model.load_state_dict(state_dict)
    print('epoch: ', torch.load(checkpoint_path)['epoch'])
    

    model = model.cuda()

    
    root_dir = "/home/cero_ma/MCV/window_benchmarks/originals/resized/ecp-ref-occ60/"
    #root_dir = "/home/cero_ma/MCV/window_benchmarks/originals/resized/cmp-ref-occ60/"
    image = cv2.imread("/home/cero_ma/MCV/window_benchmarks/originals/resized/ecp-ref-occ60/test/images/monge_31.jpg")
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    mask = cv2.imread("/home/cero_ma/MCV/window_benchmarks/originals/resized/ecp-ref-occ60/test/occ_masks/monge_31.png", cv2.IMREAD_GRAYSCALE)

    #fig, axs = plt.subplots(1, 2, figsize=(20, 10))
    #axs[0].imshow(image)
    #axs[1].imshow(mask)
    #plt.show()

    # prepare the image for the model (aligned resize)
    
    
    g_vars = global_var()
    

    test_dataset = ImageSegmentationDataset(root_dir=root_dir, g_vars= g_vars, inference= True)
    annotations = g_vars.get_var()
    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False,  collate_fn=collate_fn) #TODO: BS=3
    
    test_num = len(annotations)
    label_all = np.zeros((test_num,) + (param_dict['img_size'], param_dict['img_size']), np.uint8)
    predict_all = np.zeros((test_num,) + (param_dict['img_size'], param_dict['img_size']), np.uint8)

    
    batch = next(iter(test_dataloader))
    

    #for k,v in batch:
    #    print(k, v.shape)

    model.eval()
    pbar = tqdm(test_dataloader)
    n = 0

    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)

    meter = AverageMeter()
    optimizer.zero_grad()

    for idx, batch in enumerate(pbar):

        # get the inputs;
    
        pixel_values = batch["pixel_values"].cuda()
        mask_labels=[labels.cuda() for labels in batch["mask_labels"]]
        class_labels=[labels.cuda() for labels in batch["class_labels"]]
        labels = batch["labels"].cuda()

        #print(pixel_values.shape)

        
        outputs = model(pixel_values=pixel_values.float())# logits are of shape (batch_size, num_labels, height/4, width/4)

        upsampled_logits = nn.functional.interpolate(outputs, size=labels.shape[-2:], mode="bilinear", align_corners=False)

                
        pred = upsampled_logits.cpu().data.numpy()[:, 0, :, :]
        pred[pred >= 0.5] = 1
        pred[pred < 0.5] = 0
        pred_labels = pred.astype(np.uint32)
        
                


        #original_images = batch["original_images"]
        #target_sizes = [(image.shape[0], image.shape[1]) for image in original_images]
        #predicted_segmentation_maps = preprocessor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)
        #predicted = torch.stack(predicted_segmentation_maps, dim=0)

        
        
        

        # Second, apply argmax on the class dimension
        _seg = pred_labels
        

        for im in range(_seg.shape[0]):
            seg = _seg[im].cpu()
            label = labels[im]


            
            color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3\
            label_all[n] = seg
            
            predict_all[n] = label

            print(palette)


            for label, color in enumerate(palette):
                color_seg[seg == label, :] = color
            # Convert to BGR
            color_seg = color_seg[..., ::-1]
            
            cv2.imwrite(os.path.join(model_dir, 'test', annotations[n]), color_seg)
            n +=1



            # Show image + mask
            #img = np.array(image) * 0.5 + color_seg * 0.5
            #img = img.astype(np.uint8)

            #fig, axs = plt.subplots(1, 2, figsize=(20, 10))
            #axs[0].imshow(img)
            #axs[1].imshow(color_seg)
            #plt.show()
            
             
            #pdb.set_trace()
        
    get_acc_v2(
            label_all, predict_all,
            param_dict['num_class'] + 1 if param_dict['num_class'] == 1 else param_dict['num_class'],
            model_dir)




if len(sys.argv) == 1:
    yaml_file = 'config.yaml'
else:
    yaml_file = sys.argv[1]
param_dict = parse_yaml(yaml_file)

for kv in param_dict.items():
    print(kv)

##############PARAMS"####################################333

WIDTH = 512
HEIGHT = 512
lr = 0.00006 # param_dict['base_lr'] #0.00006 originally used
model_dir = param_dict['save_dir_model'] #'/home/cero_ma/MCV/code220419_windows/0401_files/test-SegFormer/'


model_name = '250valiou_best.pth' # '100model.pth') #

gpu= param_dict['gpu_id'] #'4,5'

os.environ["CUDA_VISIBLE_DEVICES"] = gpu
gpu_list = [i for i in range(len(gpu.split(',')))]
gx = torch.cuda.device_count()
print('useful gpu count is {}'.format(gx))


early_stopper = EarlyStopper(patience=param_dict['stop_pat'], min_delta=param_dict['stop_delta'])

transform = aug.Compose([
    aug.Flip(p=0.5),
    #aug.RandomCrop(width=128, height=128), does not work for this dataset and task
    aug.HorizontalFlip(p=0.5),
    #aug.Normalize(mean=[-0.0711,  0.3911,  0.4208], std=[1.3278, 0.6730, 0.6836]),
    aug.geometric.rotate.Rotate (limit=[-15, 15])
])

transform_train = standard_transforms.Compose([ 
            tr.RandomHorizontalFlip(),
            tr.RandomVerticalFlip(),
            tr.ScaleNRotate(rots=(-15, 15), scales=(0.9, 1.1)),
            tr.FixedResize(param_dict['img_size']),
            tr.Normalize(mean=param_dict['mean'], std=param_dict['std'])
            ])  # data pocessing and data augumentation
transform_val = standard_transforms.Compose([
        tr.FixedResize(param_dict['img_size']),
        tr.Normalize(mean=param_dict['mean'], std=param_dict['std'])
        ])  # data pocessing and data augumentation

config =   DetrConfig (
  activation_dropout= 0.0,
  activation_function= "relu",
  architectures= [
    "DetrForSegmentation" ],
  attention_dropout= 0.0,
  auxiliary_loss= False,
  backbone= "resnet50",
  bbox_cost= 5,
  bbox_loss_coefficient= 5,
  class_cost= 1,
  d_model= 256,
  decoder_attention_heads= 8,
  decoder_ffn_dim= 2048,
  decoder_layerdrop= 0.0,
  decoder_layers= 6,
  dice_loss_coefficient= 1,
  dilation= False,
  dropout= 0.1,
  encoder_attention_heads= 8,
  encoder_ffn_dim= 2048,
  encoder_layerdrop= 0.0,
  encoder_layers= 6,
  eos_coefficient= 0.1,
  giou_cost= 2,
  giou_loss_coefficient= 2,
  id2label = { 1: "occlusion"}, #0: 
  init_std= 0.02,
  init_xavier_std= 1.0,
  is_encoder_decoder= True,
  label2id = {  "occlusion": 1 }, #"background": 0,
  mask_loss_coefficient= 1,
  max_position_embeddings= 1024,
  model_type= "detr",
  num_channels= 3,
  num_hidden_layers= 6,
  num_queries= 100,
  position_embedding_type= "sine",
  scale_embedding= False,
  transformers_version= "4.26.0",
  use_pretrained_backbone= True,
  use_timm_backbone= True
    ) 
    


train()
#inference()






